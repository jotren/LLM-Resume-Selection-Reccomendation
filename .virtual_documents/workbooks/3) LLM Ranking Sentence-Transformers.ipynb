


import pandas as pd
from sentence_transformers import SentenceTransformer, util
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
from tqdm.notebook import tqdm

# Text cleaning
nltk.download('stopwords')
nltk.download('wordnet')
STOPWORDS = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text, stopwords=STOPWORDS):
    text = text.lower()
    text = re.sub(r'\b(' + r"|".join(stopwords) + r")\b\s*", '', text)
    text = re.sub(r"([!\"'#$%&()*\+,-./:;<=>?@\\\[\]^_`{|}~])", r" \1 ", text)
    text = re.sub("[^A-Za-z0-9]+", " ", text)
    text = re.sub(" +", " ", text)
    text = re.sub("\n", " ", text)
    text = re.sub(r"http\S+", "", text)
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])
    return text

# Load and preprocess the data
df = pd.read_csv("../data/UpdatedResumeDataSet.csv")

# Clean the resumes
df['cleaned_resume'] = df['resume'].apply(clean_text)

# Load Sentence Transformer model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Compute embeddings for the resumes
tqdm.pandas(desc="Creating Embeddings")
df['embeddings'] = df['cleaned_resume'].progress_apply(lambda x: model.encode(x, convert_to_tensor=True))

# Query text processing
query_text = 'Python machine learning sklearn SQL database data science optimise clean software database coding programming java javascript'
query_embedding = model.encode(query_text, convert_to_tensor=True)

# Calculate cosine similarity
df['cosine_similarity'] = df['embeddings'].progress_apply(lambda x: util.pytorch_cos_sim(query_embedding, x).item())

# Document length penalty: a more sophisticated penalty, like log transformation
df['document_length'] = df['cleaned_resume'].apply(lambda x: len(x.split()))
df['length_penalty'] = np.log1p(df['document_length'])

# Adjusted similarity with length penalty
df['adjusted_similarity'] = df['cosine_similarity'] / df['length_penalty']

# Sort by adjusted similarity score
sorted_df = df.sort_values(by='adjusted_similarity', ascending=False).reset_index(drop=True)
for index, row in sorted_df.iterrows():
    print(f"Index {index}: {row['category']} Score: {row['adjusted_similarity']}")





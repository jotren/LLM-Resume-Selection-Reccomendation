





import pandas as pd

df = pd.read_csv("../data/raw/UpdatedResumeDataSet.csv")


import json
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
import numpy as np
from transformers import BertTokenizer, BertModel
import torch
from sklearn.preprocessing import normalize
from sklearn.metrics.pairwise import cosine_similarity
from tqdm.notebook import tqdm

# Text cleaning
nltk.download('stopwords')
nltk.download('wordnet')
STOPWORDS = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def remove_duplicate_words(text):
    """
    Remove duplicate words from the text, preserving the original order.
    """
    words = text.split()
    seen = set()
    seen_add = seen.add
    # Preserve order and remove duplicates
    words_no_duplicates = [word for word in words if not (word in seen or seen_add(word))]
    return ' '.join(words_no_duplicates)

def clean_text(text, stopwords=STOPWORDS):
    """Clean raw text string."""
    # Lower
    text = text.lower()

    # Remove stopwords
    pattern = re.compile(r'\b(' + r"|".join(stopwords) + r")\b\s*")
    text = pattern.sub('', text)

    # Spacing and filters
    text = re.sub(r"([!\"'#$%&()*\+,-./:;<=>?@\\\[\]^_`{|}~])", r" \1 ", text)  # add spacing
    text = re.sub("[^A-Za-z0-9]+", " ", text)  # remove non alphanumeric chars
    text = re.sub(" +", " ", text)  # remove multiple spaces
    text = re.sub("\n", " ", text)  # remove multiple spaces
    text = text.strip()  # strip white space at the ends
    text = re.sub(r"http\S+", "", text)  #  remove links
    text = remove_duplicate_words(text)
    
    return text # Apply to dataframe

# Load BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained('bert-base-uncased')

def tokenize(text):
    encoded_inputs = tokenizer(text, return_tensors="pt", padding="longest", truncation=True, max_length=512)
    return encoded_inputs

def preprocess(df):
    df["cleaned_resume"] = df["resume"].apply(clean_text)
    df["tokenized_data"] = df["cleaned_resume"].apply(lambda x: tokenize(x))
    return df

def create_embeddings(tokenized_data):
    input_ids = tokenized_data['input_ids']
    attention_mask = tokenized_data['attention_mask']
    
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        # Average the embeddings of all tokens to get a robust representation
        embeddings = outputs.last_hidden_state.mean(dim=1)
    
    return embeddings

# Preprocess the DataFrame
processed_df = preprocess(df)
tqdm.pandas(desc="Creating Embeddings")
df['embeddings'] = df['tokenized_data'].progress_apply(lambda row: create_embeddings(row))

# Stack embeddings into a matrix
embeddings_matrix = np.vstack(df['embeddings'].values)

# Normalize embeddings
df['normalized_embeddings'] = df['embeddings'].apply(lambda x: normalize(x.reshape(1, -1), axis=1).flatten())






# Query text processing
query_text = 'Python machine learning sklearn SQL database data science database coding programming'
query_tokenized = tokenize(query_text)
query_embedding = create_embeddings(query_tokenized).numpy()

# Calculate Cosine Similarity
cosine_similarities = cosine_similarity(query_embedding.reshape(1, -1), embeddings_matrix)
df['cosine_similarity'] = cosine_similarities[0]

# Mean centering and scaling the cosine similarity scores
mean_similarity = df['cosine_similarity'].mean()
std_similarity = df['cosine_similarity'].std()
df['normalized_similarity'] = (df['cosine_similarity'] - mean_similarity) / std_similarity



# Preprocess the text for BM25
def preprocess_for_bm25(text):
    return word_tokenize(clean_text(text))

# Tokenize the resumes for BM25
df['tokenized_resume'] = df['resume'].apply(preprocess_for_bm25)

# Create a BM25 object
bm25 = BM25Okapi(df['tokenized_resume'].tolist())

# Query preprocessing for BM25
tokenized_query = preprocess_for_bm25(query_text)

# Get BM25 scores
bm25_scores = bm25.get_scores(tokenized_query)
df['bm25_score'] = bm25_scores

# Normalize BM25 scores
df['normalized_bm25_score'] = (df['bm25_score'] - df['bm25_score'].mean()) / df['bm25_score'].std()

# Adjusted similarity by combining BM25 and BERT-based cosine similarity
df['combined_similarity'] = df['normalized_similarity'] + df['normalized_bm25_score']

# Sort by combined similarity score
sorted_df = df.sort_values(by='combined_similarity', ascending=False).reset_index(drop=True)


for index, value in enumerate(sorted_df['category']):
    if index < 30:
        print(f"Index {index}: Value {value}")
    else:
        break





sorted_df.iloc[10,2]

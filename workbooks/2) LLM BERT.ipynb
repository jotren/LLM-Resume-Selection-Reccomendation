{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f260b7-6408-42bf-86e6-d73bbd7d8335",
   "metadata": {},
   "source": [
    "# BERT Tokeniser & Vector Querying\n",
    "\n",
    "The model will use a BERT tokeniser to convert the text for each CV into Semantic space and then query that space with text. Will also clean and stem the data so that it produces only the most important words. Will then use Cosine Similarity to measure the similarity in semantic space:\n",
    "\r\n",
    "For Cosine Similarity:\r\n",
    "$$\r\n",
    "\\text{CosineSimilarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}\r\n",
    "$$\r\n",
    "\r\n",
    "Where:\r\n",
    "$$\r\n",
    "A_i \\text{ and } B_i \\text{ are the components of vectors } A \\text{ and } B, \\text{ respectively.} \\\\\r\n",
    "\\|A\\| \\text{ and } \\|B\\| \\text{ are the magnitudes of vectors } A \\text{ and } B, \\text{ respectively.}\r\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9c0fc-0703-4375-a2be-15f2a2ef1a17",
   "metadata": {},
   "source": [
    "## Cleaning Word Data:\n",
    "### Stemming &Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0ee4470-d176-4126-9dcd-d0f6658e22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/raw/UpdatedResumeDataSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5760d499-800f-4a01-8374-cdd6fb38e469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7383554a-c455-4e3d-8c86-0e1f417312c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jtren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "STOPWORDS = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7f9cb65-9e0e-45a6-901d-d26670c74e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_duplicate_words(text):\n",
    "    \"\"\"\n",
    "    Remove duplicate words from the text, preserving the original order.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    # Preserve order and remove duplicates\n",
    "    words_no_duplicates = [word for word in words if not (word in seen or seen_add(word))]\n",
    "    return ' '.join(words_no_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820434db-22c6-4eee-82a6-0a56310c1452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stopwords=STOPWORDS):\n",
    "    \"\"\"Clean raw text string.\"\"\"\n",
    "    # Lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove stopwords\n",
    "    pattern = re.compile(r'\\b(' + r\"|\".join(stopwords) + r\")\\b\\s*\")\n",
    "    text = pattern.sub('', text)\n",
    "\n",
    "    # Spacing and filters\n",
    "    text = re.sub(r\"([!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)  # add spacing\n",
    "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)  # remove non alphanumeric chars\n",
    "    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n",
    "    text = re.sub(\"\\n\", \" \", text)  # remove multiple spaces\n",
    "    text = text.strip()  # strip white space at the ends\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  #  remove links\n",
    "    text = remove_duplicate_words(text)\n",
    "    \n",
    "    return text # Apply to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cead7a5-5aa8-4bb4-b8dc-9c2852952100",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtren\\anaconda3\\envs\\LLM\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\jtren\\anaconda3\\envs\\LLM\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f034e8-954c-4d1e-b62e-86197441fcf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    encoded_inputs = tokenizer(text, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=512)\n",
    "    return encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92c27ced-820c-47b1-89c6-96f46d88e075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df[\"cleaned_resume\"] = df[\"resume\"].apply(clean_text)  # Apply clean_text\n",
    "    df[\"tokenized_data\"] = df[\"cleaned_resume\"].apply(lambda x: tokenize(x))  # Apply tokenize\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af9ce9dc-82e7-4951-94e9-bee9ec244adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_df = preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898ef56f-e539-4acc-a61e-73250aa5ac11",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "We need to then create the embeddings. These are tensor objects that contain the semantic information for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17416fc7-339c-4e6f-8a55-446febafa45c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_embeddings(tokenized_data):\n",
    "    \"\"\"Generate embeddings by averaging token embeddings (excluding padding tokens).\"\"\"\n",
    "    input_ids = tokenized_data['input_ids']\n",
    "    attention_mask = tokenized_data['attention_mask']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Get the hidden states of all tokens\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        # Create a mask for ignoring padding tokens\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "        # Sum embeddings for each token, ignoring padding tokens, then divide by the number of non-padding tokens\n",
    "        sum_embeddings = torch.sum(last_hidden_states * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)  # Prevent division by zero\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "    \n",
    "    return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f060ad-7858-41a5-b862-13b4ba0fba65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas(desc=\"Creating Embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "515a2729-3c8d-4d81-8910-74fcf1f9e564",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7ab102f3a64aabb2f065d3240775f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Embeddings:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['embeddings'] = df['tokenized_data'].progress_apply(lambda row: create_embeddings(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1339b699-0b6b-4164-b92f-5288e3d0661a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "df['normalized_embeddings'] = df['embeddings'].apply(lambda x: normalize(x.reshape(1, -1), axis=1).flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3a5a72-aea8-468c-9102-16b75591a872",
   "metadata": {},
   "source": [
    "## Vector Query\n",
    "Now that we have created out embeddings, we can now query the data with a prompt. I will deliberately use a prompt that is heavily weighted towards data science. We should in theory get a lot of data science categories near the top of the list with a high similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86b62b7b-2461-4a11-b028-01df7055981d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_text = 'Python machine learning sklearn SQL database data science database coding programming'\n",
    "query_tokenized = tokenize(query_text)  # Ensure this uses the same tokenizer and method used for CVs\n",
    "query_embedding = create_embeddings(query_tokenized).numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f61e4a8a-0498-4b36-9748-ec6b693fd751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_matrix = np.vstack(df['embeddings'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43895585-f46f-4bb1-a84e-85985257cad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarities = cosine_similarity(query_embedding.reshape(1, -1), embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1067a0a-03be-48c2-9e65-f89eb61b342b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['similarity_score'] = similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc18a39e-0204-4f91-afd8-346ad9891476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values(by='similarity_score', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d45b0af-e1d1-43e5-8339-af7d6a412ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = sorted_df[['category', 'similarity_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8106cb8-6f8f-4a05-94e5-91db6fa0f100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: Value DotNet Developer\n",
      "Index 1: Value DotNet Developer\n",
      "Index 2: Value Python Developer\n",
      "Index 3: Value Python Developer\n",
      "Index 4: Value Data Science\n",
      "Index 5: Value DotNet Developer\n",
      "Index 6: Value Java Developer\n",
      "Index 7: Value Data Science\n",
      "Index 8: Value Data Science\n",
      "Index 9: Value Data Science\n",
      "Index 10: Value HR\n",
      "Index 11: Value Hadoop\n",
      "Index 12: Value Java Developer\n",
      "Index 13: Value Hadoop\n",
      "Index 14: Value Data Science\n",
      "Index 15: Value Blockchain\n",
      "Index 16: Value DevOps Engineer\n",
      "Index 17: Value Java Developer\n",
      "Index 18: Value Java Developer\n",
      "Index 19: Value Data Science\n",
      "Index 20: Value Java Developer\n",
      "Index 21: Value Python Developer\n",
      "Index 22: Value Automation Testing\n",
      "Index 23: Value Java Developer\n",
      "Index 24: Value Database\n",
      "Index 25: Value Java Developer\n",
      "Index 26: Value Python Developer\n",
      "Index 27: Value Database\n",
      "Index 28: Value Java Developer\n",
      "Index 29: Value Business Analyst\n"
     ]
    }
   ],
   "source": [
    "for index, value in enumerate(job_titles['category']):\n",
    "    if index < 30:\n",
    "        print(f\"Index {index}: Value {value}\")\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee24e71-4d3c-40ca-ae7d-c05dd21fc8f1",
   "metadata": {},
   "source": [
    "This has done a pretty good job, we can definitely see a lot of Python Developer etc. However, we have DotNet Developers at the top which indicates the algorithmn could be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd1083-d6ce-4635-971c-3e5fd4405a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf3556d-a426-4c17-a4a7-f3a8e72ccfb6",
   "metadata": {},
   "source": [
    "# BERT Tokeniser & Vector Querying (Improved)\n",
    "\n",
    "Have had a few ideas for improvements:\n",
    "\n",
    "1) Use Lemmatization instead of Stemming\n",
    "2) Take the CLS token from BERT rather than averaging the embedded values\n",
    "3) Normalise the output\n",
    "on similarity.\r\n",
    "\r\n",
    "### How Is the Enhanced Approach Better?\r\n",
    "\r\n",
    "The enhanced approach I suggested includes se\n",
    "eral__mprovements:\r\n",
    "\r\n",
    "1. **Lemmatizatio:__  of Stemming:**\r\n",
    "   - Lemmatization provides more meaningful base ~forms of words compared to stemming, which can help in improving the quality o__the embeddings.\r\n",
    "\r\n",
    ":__ Token Embedding:**\r\n",
    "   - Using the `[CLS]` token embedding captures the entire sequenceâ€™s contextual information, which can be more representative than averaging all token embeddings.\r",
    " 3nd__he compstatio:__ .\r\n",
    "\r\n",
    "4. **Normalization:**\r\n",
    "   - Embeddings are normalized to ensure that the cosine similarity measures are on a uniform scale, which can improve the accuracy of similarity scoring.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ff3ac-9b27-4373-9e61-baa06b19b643",
   "metadata": {},
   "source": [
    "## Clean and Embed Word Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931e8f8f-e032-4a89-8a86-c8e2e0a0a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/raw/UpdatedResumeDataSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba28a2c6-bc1a-4a94-ade5-b54b8df88db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtren\\anaconda3\\envs\\LLM\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jtren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jtren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\jtren\\anaconda3\\envs\\LLM\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c455b6af61354868bd321573f3d049d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Embeddings:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Text cleaning\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text, stopwords=STOPWORDS):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b(' + r\"|\".join(stopwords) + r\")\\b\\s*\", '', text)\n",
    "    text = re.sub(r\"([!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)\n",
    "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize(text):\n",
    "    encoded_inputs = tokenizer(text, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=512)\n",
    "    return encoded_inputs\n",
    "\n",
    "def preprocess(df):\n",
    "    df[\"cleaned_resume\"] = df[\"resume\"].apply(clean_text)\n",
    "    df[\"tokenized_data\"] = df[\"cleaned_resume\"].apply(lambda x: tokenize(x))\n",
    "    return df\n",
    "\n",
    "def create_embeddings(tokenized_data):\n",
    "    input_ids = tokenized_data['input_ids']\n",
    "    attention_mask = tokenized_data['attention_mask']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Average the embeddings of all tokens to get a robust representation\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Preprocess the DataFrame\n",
    "processed_df = preprocess(df)\n",
    "tqdm.pandas(desc=\"Creating Embeddings\")\n",
    "df['embeddings'] = df['tokenized_data'].progress_apply(lambda row: create_embeddings(row))\n",
    "\n",
    "# Stack embeddings into a matrix\n",
    "embeddings_matrix = np.vstack(df['embeddings'].values)\n",
    "\n",
    "# Normalize embeddings\n",
    "df['normalized_embeddings'] = df['embeddings'].apply(lambda x: normalize(x.reshape(1, -1), axis=1).flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d740c62-5800-495d-81ab-ba8e60100eef",
   "metadata": {},
   "source": [
    "## Vector Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4c98556-9527-4c4d-893d-ea45510b0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Query text processing\n",
    "query_text = 'Python machine learning sklearn SQL database data science optimise clean software database coding programming java javascript SQL'\n",
    "query_tokenized = tokenize(query_text)\n",
    "query_embedding = create_embeddings(query_tokenized).numpy()\n",
    "\n",
    "# Calculate Cosine Similarity\n",
    "cosine_similarities = cosine_similarity(query_embedding.reshape(1, -1), embeddings_matrix)\n",
    "df['cosine_similarity'] = cosine_similarities[0]\n",
    "\n",
    "# Mean centering and scaling the cosine similarity scores\n",
    "mean_similarity = df['cosine_similarity'].mean()\n",
    "std_similarity = df['cosine_similarity'].std()\n",
    "df['normalized_similarity'] = (df['cosine_similarity'] - mean_similarity) / std_similarity\n",
    "\n",
    "# Document length penalty: a more sophisticated penalty, like log transformation\n",
    "df['document_length'] = df['cleaned_resume'].apply(lambda x: len(x.split()))\n",
    "df['length_penalty'] = np.log1p(df['document_length'])\n",
    "\n",
    "# Adjusted similarity with length penalty\n",
    "df['adjusted_similarity'] = df['normalized_similarity'] - df['length_penalty']\n",
    "sorted_df = df.sort_values(by='normalized_similarity', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71a30d0b-4172-409e-80d2-390f00793905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: Value Python Developer\n",
      "Index 1: Value HR\n",
      "Index 2: Value Data Science\n",
      "Index 3: Value Java Developer\n",
      "Index 4: Value Data Science\n",
      "Index 5: Value Python Developer\n",
      "Index 6: Value Java Developer\n",
      "Index 7: Value Data Science\n",
      "Index 8: Value Hadoop\n",
      "Index 9: Value Data Science\n",
      "Index 10: Value DotNet Developer\n",
      "Index 11: Value Java Developer\n",
      "Index 12: Value Data Science\n",
      "Index 13: Value Blockchain\n",
      "Index 14: Value DotNet Developer\n",
      "Index 15: Value Hadoop\n",
      "Index 16: Value DevOps Engineer\n",
      "Index 17: Value Database\n",
      "Index 18: Value Java Developer\n",
      "Index 19: Value Automation Testing\n",
      "Index 20: Value Java Developer\n",
      "Index 21: Value Python Developer\n",
      "Index 22: Value Java Developer\n",
      "Index 23: Value Data Science\n",
      "Index 24: Value Java Developer\n",
      "Index 25: Value SAP Developer\n",
      "Index 26: Value ETL Developer\n",
      "Index 27: Value HR\n",
      "Index 28: Value Business Analyst\n",
      "Index 29: Value Automation Testing\n"
     ]
    }
   ],
   "source": [
    "for index, value in enumerate(sorted_df['category']):\n",
    "    if index < 30:\n",
    "        print(f\"Index {index}: Value {value}\")\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df83fc1b-698b-40c2-b8ee-1d517fb82e95",
   "metadata": {},
   "source": [
    "Interestingly, this has made the result worse! By making these changes, the model appears to massively favour any text that is shorter. This could be because comparing vectors with fewer words/features can sometimes produce higher similarity scores as they have fewer dimensions to normalise over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33f6a689-c46e-4e2b-881b-fd056f4e56c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'key skill computerized accounting tally sincere hard working management accounting income tax good communication leadership two four wheeler driving license internet ecommerce management computer skill c language web programing tally dbms education detail june 2017 june 2019 mba finance hr india mlrit june 2014 june 2017 bcom computer hyderabad telangana osmania university june 2012 april 2014 inter mec india srimedhav hr nani skill detail accounting exprience 6 month database management system exprience 6 month dbms exprience 6 month management accounting exprience 6 month ecommerce exprience 6 monthscompany detail company valuelabs description give rrf form required dlt hand rlt scrum master take form rlt scrum master give form trainee work requirement till candidate receive offer company'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df.iloc[27,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c11d997-1f8a-4255-85ec-de4cd417b838",
   "metadata": {},
   "source": [
    "This result came 27th in the rank despite not having a whole lot to do with python or data science."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "819a4c64-bfc6-409c-8474-5d1bbe936b1f",
   "metadata": {},
   "source": [
    "# BM25 Search\n",
    "\n",
    "In order to prevent the algorithmn from having higher cosine similarity scoes for smaller text, we can introduce the BM-25 search algorithmn.\n",
    "\r",
    "$$\n",
    "\\text{BM25}(D, Q) = \\sum_{i=1}^{n} \\text{IDF}(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k1 + 1)}{f(q_i, D) + k1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "f(q_i, D) \\text{ is the term frequency of term } q_i \\text{ in document } D. \\\\\n",
    "|D| \\text{ is the length of document } D. \\\\\n",
    "\\text{avgdl} \\text{ is the average document length in the corpus.} \\\\\n",
    "\\text{IDF}(q_i) \\text{ is the inverse document frequency of term } q_i. \\\\\n",
    "k1 \\text{ and } b \\text{ are parameters for BM25 (commonly set to 1.2 or 2.0 for } k1 \\text{ and 0.75 for } b). \\\\\n",
    "$$\n",
    "\n",
    "r \n",
    "\n",
    "This is unction that normalises for document length.} b). \\\\\r\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf3a201-ea7f-4faa-b5f8-5cea4acb97c5",
   "metadata": {},
   "source": [
    "## Clean and Embed Word Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a7ea646-cf79-4dc0-8992-8b9c91464637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/raw/UpdatedResumeDataSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76c6d50c-0979-4280-af49-0c7f090c8cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jtren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jtren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\jtren\\anaconda3\\envs\\LLM\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f404107ac9da40fcbb9988ad10fefaa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Embeddings:   0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Text cleaning\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_duplicate_words(text):\n",
    "    \"\"\"\n",
    "    Remove duplicate words from the text, preserving the original order.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    # Preserve order and remove duplicates\n",
    "    words_no_duplicates = [word for word in words if not (word in seen or seen_add(word))]\n",
    "    return ' '.join(words_no_duplicates)\n",
    "\n",
    "def clean_text(text, stopwords=STOPWORDS):\n",
    "    \"\"\"Clean raw text string.\"\"\"\n",
    "    # Lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove stopwords\n",
    "    pattern = re.compile(r'\\b(' + r\"|\".join(stopwords) + r\")\\b\\s*\")\n",
    "    text = pattern.sub('', text)\n",
    "\n",
    "    # Spacing and filters\n",
    "    text = re.sub(r\"([!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text)  # add spacing\n",
    "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)  # remove non alphanumeric chars\n",
    "    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n",
    "    text = re.sub(\"\\n\", \" \", text)  # remove multiple spaces\n",
    "    text = text.strip()  # strip white space at the ends\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  #  remove links\n",
    "    text = remove_duplicate_words(text)\n",
    "    \n",
    "    return text # Apply to dataframe\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize(text):\n",
    "    encoded_inputs = tokenizer(text, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=512)\n",
    "    return encoded_inputs\n",
    "\n",
    "def preprocess(df):\n",
    "    df[\"cleaned_resume\"] = df[\"resume\"].apply(clean_text)\n",
    "    df[\"tokenized_data\"] = df[\"cleaned_resume\"].apply(lambda x: tokenize(x))\n",
    "    return df\n",
    "\n",
    "def create_embeddings(tokenized_data):\n",
    "    input_ids = tokenized_data['input_ids']\n",
    "    attention_mask = tokenized_data['attention_mask']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Average the embeddings of all tokens to get a robust representation\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Preprocess the DataFrame\n",
    "processed_df = preprocess(df)\n",
    "tqdm.pandas(desc=\"Creating Embeddings\")\n",
    "df['embeddings'] = df['tokenized_data'].progress_apply(lambda row: create_embeddings(row))\n",
    "\n",
    "# Stack embeddings into a matrix\n",
    "embeddings_matrix = np.vstack(df['embeddings'].values)\n",
    "\n",
    "# Normalize embeddings\n",
    "df['normalized_embeddings'] = df['embeddings'].apply(lambda x: normalize(x.reshape(1, -1), axis=1).flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9394531c-2c5d-424a-b7ae-5dd01b15c272",
   "metadata": {},
   "source": [
    "## Vector Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee86e27b-85de-4174-8b04-1d032078d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query text processing\n",
    "query_text = 'Python machine learning sklearn SQL database data science database coding programming'\n",
    "query_tokenized = tokenize(query_text)\n",
    "query_embedding = create_embeddings(query_tokenized).numpy()\n",
    "\n",
    "# Calculate Cosine Similarity\n",
    "cosine_similarities = cosine_similarity(query_embedding.reshape(1, -1), embeddings_matrix)\n",
    "df['cosine_similarity'] = cosine_similarities[0]\n",
    "\n",
    "# Mean centering and scaling the cosine similarity scores\n",
    "mean_similarity = df['cosine_similarity'].mean()\n",
    "std_similarity = df['cosine_similarity'].std()\n",
    "df['normalized_similarity'] = (df['cosine_similarity'] - mean_similarity) / std_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5eaa3b45-c03a-4158-8eef-6ff7190d7adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text for BM25\n",
    "def preprocess_for_bm25(text):\n",
    "    return word_tokenize(clean_text(text))\n",
    "\n",
    "# Tokenize the resumes for BM25\n",
    "df['tokenized_resume'] = df['resume'].apply(preprocess_for_bm25)\n",
    "\n",
    "# Create a BM25 object\n",
    "bm25 = BM25Okapi(df['tokenized_resume'].tolist())\n",
    "\n",
    "# Query preprocessing for BM25\n",
    "tokenized_query = preprocess_for_bm25(query_text)\n",
    "\n",
    "# Get BM25 scores\n",
    "bm25_scores = bm25.get_scores(tokenized_query)\n",
    "df['bm25_score'] = bm25_scores\n",
    "\n",
    "# Normalize BM25 scores\n",
    "df['normalized_bm25_score'] = (df['bm25_score'] - df['bm25_score'].mean()) / df['bm25_score'].std()\n",
    "\n",
    "# Adjusted similarity by combining BM25 and BERT-based cosine similarity\n",
    "df['combined_similarity'] = df['normalized_similarity'] + df['normalized_bm25_score']\n",
    "\n",
    "# Sort by combined similarity score\n",
    "sorted_df = df.sort_values(by='combined_similarity', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9647ec32-0984-422d-9127-269f5c140277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: Value Data Science\n",
      "Index 1: Value Data Science\n",
      "Index 2: Value Data Science\n",
      "Index 3: Value Data Science\n",
      "Index 4: Value Database\n",
      "Index 5: Value Hadoop\n",
      "Index 6: Value Python Developer\n",
      "Index 7: Value Data Science\n",
      "Index 8: Value Data Science\n",
      "Index 9: Value Data Science\n",
      "Index 10: Value Python Developer\n",
      "Index 11: Value Data Science\n",
      "Index 12: Value Data Science\n",
      "Index 13: Value DotNet Developer\n",
      "Index 14: Value Python Developer\n",
      "Index 15: Value HR\n",
      "Index 16: Value Blockchain\n",
      "Index 17: Value DotNet Developer\n",
      "Index 18: Value Hadoop\n",
      "Index 19: Value DotNet Developer\n",
      "Index 20: Value Hadoop\n",
      "Index 21: Value Java Developer\n",
      "Index 22: Value DevOps Engineer\n",
      "Index 23: Value Blockchain\n",
      "Index 24: Value DotNet Developer\n",
      "Index 25: Value Java Developer\n",
      "Index 26: Value Java Developer\n",
      "Index 27: Value Automation Testing\n",
      "Index 28: Value Java Developer\n",
      "Index 29: Value Java Developer\n"
     ]
    }
   ],
   "source": [
    "for index, value in enumerate(sorted_df['category']):\n",
    "    if index < 30:\n",
    "        print(f\"Index {index}: Value {value}\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcdca78-abce-4628-a4b9-0fa32de92cfa",
   "metadata": {},
   "source": [
    "This produces a much better result. Now data science scores more highly on similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1754b4b-b8c6-4f3c-8432-9725c947a304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'technical skills responsibilities hands experience production maintenance projects handling agile methodology sdlc involved stage software development life cycle responsible gather requirement customer interaction providing estimate solution document per process fs ts coding utp utr ptf sow submission strong knowledge debugging testing based python 400 worked change controller promoting changes uat live environment pivotal cloud foundry good communication inter personal hardworking result oriented individual team certification trainings completed internal training web crawling scraping data science mongodb mysql postgresql django angular 6 html css german a1 level preparing a2 goethe institute core java ibm series course maples pune complete movex erp techn as400 rpg rpgle m3 stream serve enterprise collaborator mec education details sc computer maharashtra university b h c restful api developer kpit technologies skill flask exprience less 1 year months rest numpy 90 monthscompany company description since 6th july 2011 till date currently working 2 years support project senior 4 expertise cache memoization git pws service server standards cl ile db2 query400 sql subfiles printer files pf lf flows programs database structure mi'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df.iloc[10,2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
